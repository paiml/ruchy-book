// File: data_pipeline_optimization.ruchy
// Optimize large-scale data processing

use std::parallel;
use std::stream;

fn process_dataset(input_file, output_file) {
    let start = perf::Timer::start()
    
    // Stream processing to avoid loading all data
    let input = stream::FileStream::new(input_file)
    let output = stream::FileWriter::new(output_file)
    
    input
        // Parse in parallel
        .parallel_map(|line| {
            parse_record(line)
        }, workers: num_cpus())
        
        // Filter early to reduce data
        .filter(|record| {
            record.is_valid() && record.value > threshold
        })
        
        // Batch for efficient processing
        .batch(1000)
        
        // Process batches in parallel
        .parallel_map(|batch| {
            let enriched = enrich_batch(batch)
            let transformed = transform_batch(enriched)
            return transformed
        })
        
        // Flatten batches
        .flatten()
        
        // Write with buffering
        .for_each(|record| {
            output.write_line(to_json(record))
        })
    
    let duration = start.elapsed()
    let records = input.count()
    let throughput = records / duration.seconds()
    
    println(f"Processed {records} records in {duration}")
    println(f"Throughput: {throughput:.0} records/second")
}

// Monitor performance
let monitor = perf::Monitor::new()

monitor.track("cpu_usage", || system::cpu_usage())
monitor.track("memory_usage", || system::memory_usage())
monitor.track("disk_io", || system::disk_io_rate())

process_dataset("input.jsonl", "output.jsonl")

monitor.report()